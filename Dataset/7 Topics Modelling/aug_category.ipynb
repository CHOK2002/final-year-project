{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check each category consist how many data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the data \n",
    "file_path = \"Cleaned_News_Articles_Final.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "data = data[['text','category']]\n",
    "\n",
    "data['category'].value_counts()\n",
    "\n",
    "data.to_csv(\"original_text_category.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category of World and Entertainment aug 1 time with Back Translate technique (Too heavy) - Aborted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import nlpaug.augmenter.word as naw\n",
    "# from tqdm import tqdm\n",
    "# import random\n",
    "# import os\n",
    "# import nltk\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# # load data\n",
    "# file_path = \"Cleaned_News_Articles_Final.csv\"\n",
    "# data = pd.read_csv(file_path)\n",
    "# data = data[data['category'].isin(['world', 'entertainment'])]\n",
    "# data = data[['text', 'category']]\n",
    "\n",
    "# # minority category\n",
    "# minority_labels = {\n",
    "#     \"world\": 820,\n",
    "#     \"entertainment\": 562,\n",
    "# }\n",
    "\n",
    "# # using back translate techique to aug\n",
    "# back_translation_aug = naw.BackTranslationAug(\n",
    "#     from_model_name='Helsinki-NLP/opus-mt-en-zh',\n",
    "#     to_model_name='Helsinki-NLP/opus-mt-zh-en')\n",
    "\n",
    "# # lists to store augmented data\n",
    "# augmented_summaries = []\n",
    "# multilabels = []\n",
    "\n",
    "# for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "#     if isinstance(row['category'], str):\n",
    "#         augmented_labels = row['category'].split(',')  \n",
    "#         augmented_labels_filtered = [label.strip() for label in augmented_labels if label.strip() in minority_labels]\n",
    "        \n",
    "#         if augmented_labels_filtered:\n",
    "#             # augment the summary for in text\n",
    "#             augmented_summary_backtranslate = back_translation_aug.augment(row['text'])\n",
    "\n",
    "#             # append to the lists\n",
    "#             augmented_summaries.append(augmented_summary_backtranslate)\n",
    "#             multilabels.append(row['category'])\n",
    "\n",
    "# augmented_df = pd.DataFrame({'text': augmented_summaries, 'category': multilabels})\n",
    "\n",
    "# data_augmented_path = 'test.csv'\n",
    "# augmented_df.to_csv(data_augmented_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category of World and Entertainment aug 1 time with 1 technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChokJoe\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 41795/41795 [00:44<00:00, 933.99it/s]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# load dataset\n",
    "file_path = \"original_text_category.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# minority category\n",
    "minority_labels = {\n",
    "    \"world\": 820,\n",
    "    \"entertainment\": 562,\n",
    "}\n",
    "\n",
    "# using synonym techique to aug\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "# lists to store augmented data\n",
    "augmented_summaries = []\n",
    "multilabels = []\n",
    "\n",
    "for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    if isinstance(row['category'], str):\n",
    "        augmented_labels = row['category'].split(',')  \n",
    "        augmented_labels_filtered = [label.strip() for label in augmented_labels if label.strip() in minority_labels]\n",
    "        \n",
    "        if augmented_labels_filtered:\n",
    "            # augment the summary for in text\n",
    "            augmented_summary_synonym = aug_synonym.augment(row['text'])\n",
    "\n",
    "            # append to the lists\n",
    "            augmented_summaries.append(augmented_summary_synonym)\n",
    "            multilabels.extend([row['category']] * 1)\n",
    "\n",
    "augmented_df = pd.DataFrame({'text': augmented_summaries, 'category': multilabels})\n",
    "\n",
    "data_augmented_path = 'aug1.csv'\n",
    "augmented_df.to_csv(data_augmented_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check total of augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "world            820\n",
       "entertainment    562\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_augmented_path = \"aug1.csv\"\n",
    "augmented_df = pd.read_csv(data_augmented_path)\n",
    "augmented_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category of US, Opinions and Waether aug 4 times with 4 different techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41795/41795 [4:00:02<00:00,  2.90it/s]   \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# load data\n",
    "file_path = \"original_text_category.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# minority category\n",
    "minority_labels = {\n",
    "    \"us\": 317,\n",
    "    \"opinions\": 280,\n",
    "    \"weather\": 114,\n",
    "}\n",
    "\n",
    "# techniques to augmented\n",
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='distilbert-base-uncased', action=\"substitute\")\n",
    "\n",
    "aug2 = naw.ContextualWordEmbsAug(\n",
    "    model_path='roberta-base', action=\"substitute\")\n",
    "\n",
    "aug_insert = naw.ContextualWordEmbsAug(\n",
    "    model_path='distilbert-base-uncased', action=\"insert\")\n",
    "\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "# lists to store augmented data\n",
    "augmented_summaries = []\n",
    "multilabels = []\n",
    "\n",
    "for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    if isinstance(row['category'], str):\n",
    "        augmented_labels = row['category'].split(',') \n",
    "        augmented_labels_filtered = [label.strip() for label in augmented_labels if label.strip() in minority_labels]\n",
    "        \n",
    "        if augmented_labels_filtered:\n",
    "            # augment the summary for in headline\n",
    "            augmented_summary = aug.augment(row['text'])\n",
    "            augmented_summary2 = aug2.augment(row['text'])\n",
    "            augmented_summary_insert = aug_insert.augment(row['text'])\n",
    "            augmented_summary_synonym = aug_synonym.augment(row['text'])\n",
    "            \n",
    "            # append to the lists\n",
    "            augmented_summaries.extend([augmented_summary, augmented_summary2, augmented_summary_insert, augmented_summary_synonym])\n",
    "            multilabels.extend([row['category']] * 4)\n",
    "\n",
    "augmented_df = pd.DataFrame({'text': augmented_summaries, 'category': multilabels})\n",
    "\n",
    "data_augmented_path = 'aug2.csv'\n",
    "augmented_df.to_csv(data_augmented_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check total of augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "us          1268\n",
       "opinions    1120\n",
       "weather      456\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_augmented_path = \"aug2.csv\"\n",
    "augmented_df = pd.read_csv(data_augmented_path)\n",
    "augmented_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine both Augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# combine data\n",
    "csv_files = ['aug1.csv', 'aug2.csv']\n",
    "\n",
    "# initialize an empty list\n",
    "dfs = []\n",
    "\n",
    "# append its data to the list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "final_aug_path = \"aug_final.csv\"\n",
    "combined_data.to_csv(final_aug_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove square brackets, single quotes, and double quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = 'aug_final.csv'\n",
    "output_file = 'aug_final.csv'\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# remove square brackets, single quotes, double quotes\n",
    "df['text'] = df['text'].str.replace(r\"[\\[\\]\\\"']\", '', regex=True)\n",
    "\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Augmented data & without Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# combine data\n",
    "csv_files = ['aug_final.csv', 'original_text_category.csv']\n",
    "\n",
    "# initialize an empty list\n",
    "dfs = []\n",
    "\n",
    "# append its data to the list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "final_aug_path = \"augted_category.csv\"\n",
    "combined_data.to_csv(final_aug_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling to select each category 1000 data (Balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChokJoe\\AppData\\Local\\Temp\\ipykernel_23084\\2568672057.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  downsampled_data = df.groupby('category').apply(lambda x: x.sample(n=min(upper_limit, max(lower_limit, len(x))), random_state=42))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "category\n",
       "business         1000\n",
       "entertainment    1000\n",
       "health           1000\n",
       "news             1000\n",
       "opinions         1000\n",
       "politics         1000\n",
       "sport            1000\n",
       "us               1000\n",
       "world            1000\n",
       "weather           570\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv('augted_category.csv')  \n",
    "\n",
    "# select 10000 data for each sentiment\n",
    "lower_limit = 100\n",
    "upper_limit = 1000\n",
    "\n",
    "# grp the data by sentiment and perform downsampling within each grp\n",
    "downsampled_data = df.groupby('category').apply(lambda x: x.sample(n=min(upper_limit, max(lower_limit, len(x))), random_state=42))\n",
    "\n",
    "downsampled_data = downsampled_data.reset_index(drop=True)\n",
    "\n",
    "downsampled_data.to_csv('balanced_category.csv', index=False)\n",
    "downsampled_data['category'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
