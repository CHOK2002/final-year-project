{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexicon Method with VADER, AFINN and Textblob technique (Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # load the dataset\n",
    "# df = pd.read_csv('Cleaned_News_Articles_Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[['headline']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# # vader sentiment analyzer\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # classify sentiment \n",
    "# def classify_sentiment(text):\n",
    "#     scores = analyzer.polarity_scores(text)\n",
    "#     compound = scores['compound']\n",
    "\n",
    "#     if compound >= 0.05:\n",
    "#         sentiment = 'positive'\n",
    "#     elif compound <= -0.05:\n",
    "#         sentiment = 'negative'\n",
    "#     else:\n",
    "#         sentiment = 'neutral'\n",
    "\n",
    "#     return sentiment, compound\n",
    "\n",
    "# # # WIthout NEUTRAl\n",
    "\n",
    "# # # classify sentiment without NEUTRAL\n",
    "# # def classify_sentiment(text):\n",
    "# #     # sentiment scores\n",
    "# #     scores = analyzer.polarity_scores(text)\n",
    "# #     # compound score\n",
    "# #     compound = scores['compound']\n",
    "# #     # sentiment based on the compound score\n",
    "# #     if compound >= 0:\n",
    "# #         return 'positive'\n",
    "# #     else:\n",
    "# #         return 'negative'\n",
    "\n",
    "# # df['sentiment'] = df['headline'].apply(classify_sentiment)\n",
    "# df['sentiment'], df['sentiment_score'] = zip(*df['headline'].apply(classify_sentiment))\n",
    "\n",
    "# print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AFINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from afinn import Afinn\n",
    "\n",
    "# # afinn sentiment analyzer\n",
    "# afinn = Afinn()\n",
    "\n",
    "# # classify sentiment\n",
    "# def classify_sentiment(text):\n",
    "#     # sentiment score\n",
    "#     score = afinn.score(text)\n",
    "#     # sentiment based on the score\n",
    "#     if score > 0:\n",
    "#         return 'positive'\n",
    "#     elif score < 0:\n",
    "#         return 'negative'\n",
    "#     else:\n",
    "#         return 'neutral'\n",
    "    \n",
    "# def calculate_sentiment_score(text):\n",
    "#     return afinn.score(text)\n",
    "\n",
    "# # WIthout NEUTRAl\n",
    "\n",
    "# # classify sentiment\n",
    "# def classify_sentiment(text):\n",
    "#     # sentiment score\n",
    "#     score = afinn.score(text)\n",
    "#     # sentiment based on the score\n",
    "#     if score > 0:\n",
    "#         return 'positive'\n",
    "#     else:\n",
    "#         return 'negative'\n",
    "\n",
    "# def calculate_sentiment_score(text):\n",
    "#     return afinn.score(text)\n",
    "\n",
    "# df['sentiment'] = df['headline'].apply(classify_sentiment)\n",
    "# df['sentiment_score'] = df['headline'].apply(calculate_sentiment_score)\n",
    "# print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# # classify sentiment\n",
    "# def classify_sentiment(text):\n",
    "#     analysis = TextBlob(text)\n",
    "#     polarity = analysis.sentiment.polarity\n",
    "    \n",
    "#     if polarity > 0:\n",
    "#         sentiment = 'positive'\n",
    "#     elif polarity < 0:\n",
    "#         sentiment = 'negative'\n",
    "#     else:\n",
    "#         sentiment = 'neutral'\n",
    "    \n",
    "#     return sentiment, polarity\n",
    "\n",
    "# # # WIthout NEUTRAl\n",
    "\n",
    "# # # classify sentiment\n",
    "# # def classify_sentiment(text):\n",
    "# #     analysis = TextBlob(text)\n",
    "# #     polarity = analysis.sentiment.polarity\n",
    "    \n",
    "# #     if polarity > 0:\n",
    "# #         return 'positive'\n",
    "# #     else:\n",
    "# #         return 'negative'\n",
    "\n",
    "# # df['sentiment'] = df['headline'].apply(classify_sentiment)\n",
    "# df['sentiment'], df['sentiment_score'] = zip(*df['headline'].apply(classify_sentiment))\n",
    "# print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"textblob_without_neutral.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Classifier with ML models (LR, SVM, MNB, RF, KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the dataset\n",
    "dataset_path = 'textblob_without_neutral.csv'    #can CHANGE\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into training and testing sets (70 30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['headline'], df['sentiment'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encode labels (negative=0 positive=1 neutral=2)\n",
    "# y_train = y_train.map({'negative': 0, 'positive': 1, 'neutral': 2})\n",
    "# y_test = y_test.map({'negative': 0, 'positive': 1, 'neutral': 2})\n",
    "\n",
    "# encode labels without NEUTRAL\n",
    "y_train = y_train.map({'negative': 0, 'positive': 1})\n",
    "y_test = y_test.map({'negative': 0, 'positive': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF vector\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "# ML baseline model (select 1 to run)    #can CHANGE\n",
    "# ml_models = LogisticRegression()\n",
    "# ml_models = RandomForestClassifier()\n",
    "# ml_models = MultinomialNB()\n",
    "# ml_models = SVC()\n",
    "ml_models = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "ml_models.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# convert model name to string\n",
    "model_name = ml_models.__class__.__name__\n",
    "\n",
    "# start MLflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"sentiment-analysis-lexicon\")\n",
    "\n",
    "# initialize MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "# experiment ID\n",
    "experiment_id = client.get_experiment_by_name(\"sentiment-analysis-lexicon\").experiment_id\n",
    "\n",
    "runs = client.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "# initial version to 0\n",
    "max_version = 0\n",
    "\n",
    "# find the max version for the current version model\n",
    "for run in runs:\n",
    "    run_name = run.data.tags.get('mlflow.runName')\n",
    "    if run_name and run_name.startswith(model_name):        \n",
    "        # extract version number from the run name\n",
    "        try:\n",
    "            version = int(run_name.split('_v')[-1])\n",
    "        except ValueError:\n",
    "            continue  # skip if version is not an integer\n",
    "\n",
    "        # update max_version if this version is greater\n",
    "        max_version = max(max_version, version)\n",
    "\n",
    "# increase the version by adding 1\n",
    "new_version = max_version + 1\n",
    "\n",
    "# new run name\n",
    "new_run_name = f\"{model_name}_v{new_version}\"\n",
    "mlflow.start_run(run_name=new_run_name)\n",
    "\n",
    "mlflow.log_param(\"model name\", model_name)\n",
    "mlflow.log_param(\"dataset_name\", dataset_path)\n",
    "mlflow.log_param(\"data size\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict accuracy (Based model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# predict test \n",
    "y_pred = ml_models.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Based Model:\")\n",
    "\n",
    "#  training accuracy\n",
    "trainAccuracy = ml_models.score(X_train_tfidf, y_train)\n",
    "print(\"Training Accuracy:\", trainAccuracy)\n",
    "\n",
    "# training loss (MISC)\n",
    "training_loss = ml_models.score(X_train_tfidf, y_train)\n",
    "print(\"Training Loss (MISC):\", 1 - training_loss)\n",
    "\n",
    "# test accuracy\n",
    "testAccuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Testing Accuracy:\", testAccuracy)\n",
    "\n",
    "# precison recall f1score\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix (Based model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# # decode the labels back to original\n",
    "# reverse_mapping = {0: 'negative', 1: 'positive', 2: 'neutral'}\n",
    "# decoded_y_pred = [reverse_mapping[label] for label in y_pred]\n",
    "# decoded_y_test = [reverse_mapping[label] for label in y_test]\n",
    "\n",
    "# # create confusion matrix\n",
    "# conf_matrix = confusion_matrix(decoded_y_test, decoded_y_pred)\n",
    "\n",
    "# # plot confusion matrix \n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "# plt.xlabel('Predicted Labels')\n",
    "# plt.ylabel('True Labels')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.xticks(ticks=[0.5, 1.5, 2.5], labels=['negative', 'positive', 'neutral'])\n",
    "# plt.yticks(ticks=[0.5, 1.5, 2.5], labels=['negative', 'positive', 'neutral'])\n",
    "# plt.show()\n",
    "\n",
    "# Without NEUTRAL\n",
    "\n",
    "# decode the labels back to original\n",
    "reverse_mapping = {0: 'negative', 1: 'positive'}\n",
    "decoded_y_pred = [reverse_mapping[label] for label in y_pred]\n",
    "decoded_y_test = [reverse_mapping[label] for label in y_test]\n",
    "\n",
    "# create confusion matrix\n",
    "conf_matrix = confusion_matrix(decoded_y_test, decoded_y_pred)\n",
    "\n",
    "# plot confusion matrix \n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(ticks=[0.5, 1.5], labels=['negative', 'positive'])\n",
    "plt.yticks(ticks=[0.5, 1.5], labels=['negative', 'positive'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curve (Based model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # binarize the labels\n",
    "# y_test_binarized = label_binarize(y_test, classes=[0, 1, 2])\n",
    "\n",
    "# # proba for proba models , decision for svm\n",
    "# y_pred_scores = ml_models.predict_proba(X_test_tfidf)       #can CHANGE\n",
    "# # y_pred_scores = ml_models.decision_function(X_test_tfidf)   \n",
    "\n",
    "# # compute and plot ROC curve\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "# n_classes = y_test_binarized.shape[1]\n",
    "\n",
    "# for i in range(n_classes):\n",
    "#     fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_scores[:, i])\n",
    "#     roc_auc[i] = roc_auc_score(y_test_binarized[:, i], y_pred_scores[:, i])\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "# colors = ['blue', 'red', 'green']\n",
    "# classes = ['negative', 'positive', 'neutral']\n",
    "# for i, color in enumerate(colors):\n",
    "#     plt.plot(fpr[i], tpr[i], color=color, lw=2, \n",
    "#              label=f'ROC curve for {classes[i]} (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic (ROC) Curve for Multi-class')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n",
    "\n",
    "# Without Neutral\n",
    "\n",
    "# ROC curve \n",
    "y_pred_scores = ml_models.predict_proba(X_test_tfidf)[:, 1]    #can CHANGE\n",
    "# y_pred_scores = ml_models.decision_function(X_test_tfidf)   \n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_scores)\n",
    "auc_score = roc_auc_score(y_test, y_pred_scores)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_score)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for Fine-tuning models (Hypertuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# ML models classifier                 #can CHANGE\n",
    "# ml_models = LogisticRegression()  \n",
    "# ml_models = RandomForestClassifier()\n",
    "# ml_models = MultinomialNB()\n",
    "# ml_models = SVC()\n",
    "ml_models = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "# hyperparameters to tune    #can CHANGE\n",
    "\n",
    "# #LR\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1.0, 10.0],\n",
    "#     'penalty': ['l1', 'l2'],\n",
    "#     'solver': ['liblinear', 'saga']\n",
    "# }\n",
    "\n",
    "# #RF\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'max_features': ['auto', 'sqrt'],\n",
    "#     'max_depth': [10, 20],\n",
    "#     'min_samples_split': [2, 5],\n",
    "#     'min_samples_leaf': [1, 2],\n",
    "#     'bootstrap': [True]\n",
    "# }\n",
    "\n",
    "# #MNB\n",
    "# param_grid = {\n",
    "#     'alpha': [0.1, 0.5, 1.0],\n",
    "#     'fit_prior': [True, False]\n",
    "# }\n",
    "\n",
    "# #SVM\n",
    "# param_grid = {\n",
    "#     'C': [1, 5],\n",
    "#     'kernel': ['linear', 'poly'],\n",
    "#     'gamma': ['scale', 'auto']\n",
    "# }\n",
    "\n",
    "# KNN\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "\n",
    "# perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=ml_models, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# fit the grid search to the data\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# best hyperparameters and corresponding score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Cross-Validation Accuracy:\", best_score)\n",
    "\n",
    "# make predictions\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions_test = best_model.predict(X_test_tfidf)\n",
    "\n",
    "# calculate accuracy \n",
    "best_model_train_score = best_model.score(X_train_tfidf, y_train)\n",
    "best_model_test_score = best_model.score(X_test_tfidf, y_test)\n",
    "\n",
    "print()\n",
    "print(\"Best Model:\")\n",
    "print(\"Training Score: {}\\nTest Score: {}\".format(best_model_train_score, best_model_test_score))\n",
    "\n",
    "# classification report\n",
    "report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions_test))\n",
    "\n",
    "mlflow.log_param(\"best params\", best_params)\n",
    "\n",
    "mlflow.log_metric(\"train accuracy\", best_model_train_score)\n",
    "mlflow.log_metric(\"test accuracy\", best_model_test_score)\n",
    "\n",
    "mlflow.log_metric(\"Precision\", report['weighted avg']['precision'])\n",
    "mlflow.log_metric(\"Recall\", report['weighted avg']['recall'])\n",
    "mlflow.log_metric(\"F1 Score\", report['weighted avg']['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix (Best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# # decode the labels back to original\n",
    "# reverse_mapping = {0: 'negative', 1: 'positive', 2: 'neutral'}\n",
    "# decoded_y_pred = [reverse_mapping[label] for label in predictions_test]\n",
    "# decoded_y_test = [reverse_mapping[label] for label in y_test]\n",
    "\n",
    "# # create confusion matrix\n",
    "# conf_matrix = confusion_matrix(decoded_y_test, decoded_y_pred)\n",
    "\n",
    "# # plot confusion matrix \n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "# plt.xlabel('Predicted Labels')\n",
    "# plt.ylabel('True Labels')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.xticks(ticks=[0.5, 1.5, 2.5], labels=['negative', 'positive', 'neutral'])\n",
    "# plt.yticks(ticks=[0.5, 1.5, 2.5], labels=['negative', 'positive', 'neutral'])\n",
    "# plt.show()\n",
    "\n",
    "# Without NEUTRAL\n",
    "\n",
    "# decode the labels back to original\n",
    "reverse_mapping = {0: 'negative', 1: 'positive'}\n",
    "decoded_y_pred = [reverse_mapping[label] for label in predictions_test]\n",
    "decoded_y_test = [reverse_mapping[label] for label in y_test]\n",
    "\n",
    "# create confusion matrix\n",
    "conf_matrix = confusion_matrix(decoded_y_test, decoded_y_pred)\n",
    "\n",
    "# plot confusion matrix \n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(ticks=[0.5, 1.5], labels=['negative', 'positive'])\n",
    "plt.yticks(ticks=[0.5, 1.5], labels=['negative', 'positive'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curve (Best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # binarize the labels\n",
    "# y_test_binarized = label_binarize(y_test, classes=[0, 1, 2])\n",
    "\n",
    "\n",
    "# # proba for proba models , decision for svm\n",
    "# y_pred_scores = best_model.predict_proba(X_test_tfidf)        #can CHANGE\n",
    "# # y_pred_scores = best_model.decision_function(X_test_tfidf)\n",
    "\n",
    "# # compute and plot ROC curve\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "# n_classes = y_test_binarized.shape[1]\n",
    "\n",
    "# for i in range(n_classes):\n",
    "#     fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_scores[:, i])\n",
    "#     roc_auc[i] = roc_auc_score(y_test_binarized[:, i], y_pred_scores[:, i])\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "# colors = ['blue', 'red', 'green']\n",
    "# classes = ['negative', 'positive', 'neutral']\n",
    "# for i, color in enumerate(colors):\n",
    "#     plt.plot(fpr[i], tpr[i], color=color, lw=2, \n",
    "#              label=f'ROC curve for {classes[i]} (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic (ROC) Curve for Multi-class')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Without Neutral\n",
    "\n",
    "# ROC curve \n",
    "y_pred_scores = best_model.predict_proba(X_test_tfidf)[:, 1]   #can CHANGE\n",
    "# y_pred_scores = best_model.decision_function(X_test_tfidf)   \n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_scores)\n",
    "auc_score = roc_auc_score(y_test, y_pred_scores)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_score)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save best model and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "# create subfolders\n",
    "base_folder = '5 Unsupervised Sentiment Analysis/lexicon_models_fextract'\n",
    "models_folder = os.path.join(base_folder, 'models')\n",
    "feature_extract_folder = os.path.join(base_folder, 'feature_extract')\n",
    "\n",
    "# double confirm \n",
    "os.makedirs(models_folder, exist_ok=True)\n",
    "os.makedirs(feature_extract_folder, exist_ok=True)\n",
    "\n",
    "# save best model and TFIDF vectorizer \n",
    "joblib.dump(best_model, os.path.join(models_folder, 'best_knn_model_textblob_without_neutral.pkl'))    #CHANGE model name and dataset name\n",
    "print('Best model saved')\n",
    "\n",
    "joblib.dump(tfidf_vectorizer, os.path.join(feature_extract_folder, 'knn_tfidf_textblob_without_neutral.pkl'))    #CHANGE also\n",
    "print('TFIDF saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Test Unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# load the saved model and tfidf vectorizer\n",
    "model_filename = \"5 Unsupervised Sentiment Analysis/lexicon_models_fextract/models/best_knn_model_textblob_without_neutral.pkl\"\n",
    "vectorizer_filename = \"5 Unsupervised Sentiment Analysis/lexicon_models_fextract/feature_extract/knn_tfidf_textblob_without_neutral.pkl\"\n",
    "\n",
    "model = joblib.load(model_filename)\n",
    "vectorizer = joblib.load(vectorizer_filename)\n",
    "\n",
    "# encode the label\n",
    "label_encoding = {0: 'negative', 1: 'positive', 2: 'neutral'}\n",
    "# label_encoding = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "\n",
    "# unseen data\n",
    "# input_text = \"McDonald’s shortens breakfast time in Australia as bird flu causes egg shortage\"\n",
    "# input_text = \"Tesla sales fall again as more automakers crowd electric vehicle market\"\n",
    "input_text = \"today is a bad day\"\n",
    "\n",
    "\n",
    "# vector the unseen data\n",
    "transformed_input = vectorizer.transform([input_text])\n",
    "\n",
    "# predit the unseen data by usig saved model\n",
    "prediction = model.predict(transformed_input)\n",
    "\n",
    "# decode the label into its original class\n",
    "decoded_prediction = label_encoding[prediction[0]]\n",
    "\n",
    "# print result\n",
    "print(f\"Prediction: {decoded_prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
