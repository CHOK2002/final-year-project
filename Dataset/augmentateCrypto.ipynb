{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Duplicates and drop / Check each labels got how many data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the CSV file\n",
    "file_path = \"Compiled_ALL_Cryptocurrency.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove duplicate rows based on the \"text\" column\n",
    "data.drop_duplicates(subset=\"text\", keep=\"first\", inplace=True)\n",
    "\n",
    "# Select specific columns\n",
    "data = data[[\"text\", \"annotation\"]]\n",
    "\n",
    "# Drop rows with any missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Save the DataFrame back to the CSV file, overwriting the original file\n",
    "data.to_csv(file_path, index=False)\n",
    "\n",
    "# Display the value counts of 'annotation'\n",
    "data['annotation'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop minority labels and save to a new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the original CSV file\n",
    "# input_csv = 'Compiled_ALL_Cryptocurrency.csv'\n",
    "# df = pd.read_csv(input_csv)\n",
    "\n",
    "# # Define a list of terms to be dropped\n",
    "# terms_to_drop = [\n",
    "#     'White Paper',\n",
    "#     'Hard Fork',\n",
    "#     'Proof-of-Concept',\n",
    "#     'Minting',\n",
    "#     'Fiat Currency',\n",
    "#     'Airdrop',\n",
    "#     'Money Laundering',\n",
    "#     'TradFi',\n",
    "#     'Multichain',\n",
    "#     'Proof-of-Stake',\n",
    "#     'Proof-of-Work',\n",
    "#     'Crypto Winter',\n",
    "#     'Proof-of-Reserves'\n",
    "# ]\n",
    "\n",
    "# # Filter the rows where 'annotation' does not contain any of the specified terms\n",
    "# df_filtered = df[~df['annotation'].isin(terms_to_drop)]\n",
    "\n",
    "# # Save the filtered data to a new CSV file\n",
    "# output_csv = 'filtered_data.csv'\n",
    "# df_filtered.to_csv(output_csv, index=False)\n",
    "# df_filtered = data\n",
    "# print(f\"Filtered data saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the augmented models can use anot or logic anot / Provided example text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "aug = naw.ContextualWordEmbsAug(model_path='distilroberta-base', action=\"substitute\", device='cuda')\n",
    "\n",
    "aug_sub = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\", device='cuda')\n",
    "\n",
    "aug_sub2 = naw.ContextualWordEmbsAug(model_path='roberta-base', action=\"insert\", device='cuda')\n",
    "\n",
    "# Original sentence\n",
    "original_sentence = \"For space id, the tokens going toward the ecosystem fund, community airdrop, and the foundation will likely not be sold immediately for a profit.\"\n",
    "\n",
    "# Augmenting using substitute action\n",
    "augmented_sentence_substitute1= aug.augment(original_sentence)\n",
    "\n",
    "# Augmenting using insert action\n",
    "augmented_sentence_substitute2 = aug_sub.augment(original_sentence)\n",
    "\n",
    "# Augmenting using insert action\n",
    "augmented_sentence_substitute3 = aug_sub2.augment(original_sentence)\n",
    "\n",
    "print(\"Substitute Augmented Sentence:\")\n",
    "print(augmented_sentence_substitute1)\n",
    "print(\"\\Substitute Augmented Sentence:\")\n",
    "print(augmented_sentence_substitute2)\n",
    "print(\"\\Substitute Augmented Sentence:\")\n",
    "print(augmented_sentence_substitute3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmented the minority labels of each data for 3 times as using 3 diff models then save to augmented_data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"# Load your dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define the minority labels and their respective counts\n",
    "minority_labels = {\n",
    "    \"Money Laundering\": 177,\n",
    "    \"Multichain\": 134,\n",
    "    \"Proof-of-Concept\": 109,\n",
    "    \"Fiat Currency\": 108,\n",
    "    \"Proof-of-Stake\": 52,\n",
    "    \"Proof-of-Reserves\": 29,\n",
    "    \"Proof-of-Work\": 21,\n",
    "}\n",
    "\n",
    "# Initialize the augmenter\n",
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='distilroberta-base', action=\"substitute\", device='cuda')\n",
    "\n",
    "back_translation_aug = naw.BackTranslationAug(\n",
    "    from_model_name='Helsinki-NLP/opus-mt-en-zh',\n",
    "    to_model_name='Helsinki-NLP/opus-mt-zh-en', device='cuda')\n",
    "\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "# Create lists to store augmented data\n",
    "augmented_summaries = []\n",
    "multilabels = []\n",
    "\n",
    "for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    if isinstance(row['annotation'], str):\n",
    "        augmented_labels = row['annotation'].split(',')  # Assuming labels are separated by commas\n",
    "        augmented_labels_filtered = [label.strip() for label in augmented_labels if label.strip() in minority_labels]\n",
    "        \n",
    "        if augmented_labels_filtered:\n",
    "            # Augment the summary for this row\n",
    "            augmented_summary = aug.augment(row['text'])\n",
    "            augmented_summary_backtranslate = back_translation_aug.augment(row['text'])\n",
    "            augmented_summary_synonym = aug_synonym.augment(row['text'])\n",
    "            # Append the augmented summary and corresponding multilabel to the lists\n",
    "         \n",
    "            augmented_summaries.append(augmented_summary)\n",
    "            augmented_summaries.append(augmented_summary_backtranslate)\n",
    "            augmented_summaries.append(augmented_summary_synonym)\n",
    "            multilabels.append(row['annotation'])\n",
    "            multilabels.append(row['annotation'])\n",
    "            multilabels.append(row['annotation'])\n",
    "\n",
    "    # Create a new DataFrame with Augmented_Summary and Multilabel columns\n",
    "    augmented_df = pd.DataFrame({'text': augmented_summaries, 'annotation': multilabels})\n",
    "\n",
    "# Save the augmented DataFrame to a new CSV file\n",
    "data_augmented_path = 'augmented_data.csv'\n",
    "augmented_df.to_csv(data_augmented_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check total had augmented how many data for the minority labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmented_path = \"augmented_data.csv\"\n",
    "augmented_df = pd.read_csv(data_augmented_path)\n",
    "augmented_df['annotation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the augmented data labels still lesser, pick those lesser and augmented 2 more times as using 2 diff models and save to augmented2_data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define the minority labels and their respective counts\n",
    "minority_labels = {\n",
    "    \"Proof-of-Stake\": 52,\n",
    "    \"Proof-of-Reserves\": 29,\n",
    "    \"Proof-of-Work\": 21,\n",
    "}\n",
    "\n",
    "# Initialize the augmenter\n",
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased', action=\"substitute\", device='cuda')\n",
    "\n",
    "aug_insert = naw.ContextualWordEmbsAug(\n",
    "    model_path='roberta-base', action=\"insert\", device='cuda')\n",
    "\n",
    "# Create lists to store augmented data\n",
    "augmented_summaries = []\n",
    "multilabels = []\n",
    "\n",
    "for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    if isinstance(row['annotation'], str):\n",
    "        augmented_labels = row['annotation'].split(',')  # Assuming labels are separated by commas\n",
    "        augmented_labels_filtered = [label.strip() for label in augmented_labels if label.strip() in minority_labels]\n",
    "\n",
    "        if augmented_labels_filtered:\n",
    "            # Augment the summary for this row\n",
    "            augmented_summary = aug.augment(row['text'])\n",
    "            augmented_summary_insert = aug_insert.augment(row['text'])\n",
    "            # Append the augmented summary and corresponding multilabel to the lists\n",
    "            augmented_summaries.append(augmented_summary)\n",
    "            augmented_summaries.append(augmented_summary_insert)\n",
    "            multilabels.append(row['annotation'])\n",
    "            multilabels.append(row['annotation'])\n",
    "\n",
    "# Create a new DataFrame with Augmented_Summary and Multilabel columns\n",
    "augmented2_df = pd.DataFrame({'text': augmented_summaries, 'annotation': multilabels})\n",
    "\n",
    "# Save the augmented DataFrame to a new CSV file\n",
    "data_augmented2_path = 'augmented2_data.csv'\n",
    "augmented2_df.to_csv(data_augmented2_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check the minority labels had hit target anot, if still not then just proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmented2_path = \"augmented2_data.csv\"\n",
    "augmented2_df = pd.read_csv(data_augmented2_path)\n",
    "augmented2_df['annotation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the augmented_data and augmented2_data into augmented_final data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of CSV file names to combine\n",
    "csv_files = ['augmented_data.csv', 'augmented2_data.csv']\n",
    "\n",
    "# Initialize an empty list to store the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each CSV file and append its data to the list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate the DataFrames in the list\n",
    "combined_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Sort the combined data based on the \"annotation\" column\n",
    "sorted_data = combined_data.sort_values(by='annotation')\n",
    "\n",
    "# Save the sorted data to a new CSV file\n",
    "final_aug_path = \"augmented_final.csv\"\n",
    "sorted_data.to_csv(final_aug_path, index=False)\n",
    "\n",
    "sorted_data['annotation'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the original dataset into 3 sets (Training Validating and Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Suppose 'data' is your original dataframe\n",
    "\n",
    "# Split the data into train and temp sets\n",
    "train_data, temp_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Save the 90% training dataset to a new CSV file\n",
    "train_path = \"training_data.csv\"\n",
    "train_data.to_csv(train_path, index=False)\n",
    "\n",
    "# Split the temp data into test and validation sets\n",
    "test_data, val_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save the test dataset to a new CSV file\n",
    "test_path = \"test_data.csv\"\n",
    "test_data.to_csv(test_path, index=False)\n",
    "\n",
    "# Save the validation dataset to a new CSV file\n",
    "val_path = \"validation_data.csv\"\n",
    "val_data.to_csv(val_path, index=False)\n",
    "\n",
    "# Retrieve the value counts of the 'annotation' column in the train_data dataframe\n",
    "value_counts = train_data['annotation'].value_counts()\n",
    "\n",
    "# Print the value counts\n",
    "print(value_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the training data and augmented_final data into train_data_augmented data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of CSV file names to combine\n",
    "csv_files = ['training_data.csv', 'augmented_final.csv']\n",
    "\n",
    "# Initialize an empty list to store the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each CSV file and append its data to the list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate the DataFrames in the list\n",
    "combined_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Sort the combined data based on the \"annotation\" column\n",
    "sorted2_data = combined_data.sort_values(by='annotation')\n",
    "\n",
    "# Save the sorted data to a new CSV file\n",
    "final_aug_path = \"train_data_augmented.csv\"\n",
    "sorted2_data.to_csv(final_aug_path, index=False)\n",
    "\n",
    "sorted2_data['annotation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling the train_data_augmented csv into all data having equal labels and save to a new csv final_train_data_augmented data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('train_data_augmented.csv')  # Replace 'path_to_your_file.csv' with the actual path to your CSV file\n",
    "\n",
    "# Define the range for the random selection for each annotation\n",
    "lower_limit = 100\n",
    "upper_limit = 200\n",
    "\n",
    "# Group the data by annotation and perform downsampling within each group\n",
    "downsampled_data = df.groupby('annotation').apply(lambda x: x.sample(n=min(upper_limit, max(lower_limit, len(x))), random_state=42))\n",
    "\n",
    "# Reset the index of the downsampled data\n",
    "downsampled_data = downsampled_data.reset_index(drop=True)\n",
    "\n",
    "# Write the downsampled data to a new CSV file\n",
    "downsampled_data.to_csv('final_train_data_augmented.csv', index=False)  # The 'index=False' parameter is used to not write row indices\n",
    "\n",
    "# Print the downsampled data\n",
    "downsampled_data['annotation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove brackets single quotes and double quotes on the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'data.csv' with the path to your CSV file\n",
    "input_file = 'final_train_data_augmented.csv'\n",
    "output_file = 'final_train_data_augmented.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Clean the 'text' column by removing square brackets, single quotes, and double quotes\n",
    "df['text'] = df['text'].str.replace(r\"[\\[\\]\\\"']\", '', regex=True)\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "df.to_csv(output_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
