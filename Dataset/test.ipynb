{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating example documents\n",
    "doc_1 = \"A whopping 96.5 percent of water on Earth is in our oceans, covering 71 percent of the surface of our planet. And at any given time, about 0.001 percent is floating above us in the atmosphere. If all of that water fell as rain at once, the whole planet would get about 1 inch of rain.\"\n",
    "\n",
    "doc_2 = \"One-third of your life is spent sleeping. Sleeping 7-9 hours each night should help your body heal itself, activate the immune system, and give your heart a break. Beyond that--sleep experts are still trying to learn more about what happens once we fall asleep.\"\n",
    "\n",
    "doc_3 = \"A newborn baby is 78 percent water. Adults are 55-60 percent water. Water is involved in just about everything our body does.\"\n",
    "\n",
    "doc_4 = \"While still in high school, a student went 264.4 hours without sleep, for which he won first place in the 10th Annual Great San Diego Science Fair in 1964.\"\n",
    "\n",
    "doc_5 = \"We experience water in all three states: solid ice, liquid water, and gas water vapor.\"\n",
    "\n",
    "# Create corpus\n",
    "corpus = [doc_1, doc_2, doc_3, doc_4, doc_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ChokJoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ChokJoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ChokJoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Code source: https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')  \n",
    "nltk.download('omw-1.4')  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# remove stopwords, punctuation, and normalize the corpus\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "clean_corpus = [clean(doc).split() for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['whopping', '965', 'percent', 'water', 'earth', 'ocean', 'covering', '71', 'percent', 'surface', 'planet', 'given', 'time', '0001', 'percent', 'floating', 'u', 'atmosphere', 'water', 'fell', 'rain', 'once', 'whole', 'planet', 'would', 'get', '1', 'inch', 'rain'], ['onethird', 'life', 'spent', 'sleeping', 'sleeping', '79', 'hour', 'night', 'help', 'body', 'heal', 'itself', 'activate', 'immune', 'system', 'give', 'heart', 'break', 'beyond', 'thatsleep', 'expert', 'still', 'trying', 'learn', 'happens', 'fall', 'asleep'], ['newborn', 'baby', '78', 'percent', 'water', 'adult', '5560', 'percent', 'water', 'water', 'involved', 'everything', 'body', 'doe'], ['still', 'high', 'school', 'student', 'went', '2644', 'hour', 'without', 'sleep', 'first', 'place', '10th', 'annual', 'great', 'san', 'diego', 'science', 'fair', '1964'], ['experience', 'water', 'three', 'state', 'solid', 'ice', 'liquid', 'water', 'gas', 'water', 'vapor']]\n"
     ]
    }
   ],
   "source": [
    "print(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Creating document-term matrix \n",
    "dictionary = corpora.Dictionary(clean_corpus)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in clean_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# LDA model\n",
    "lda = LdaModel(doc_term_matrix, num_topics=3, id2word = dictionary)\n",
    "\n",
    "# Results\n",
    "print(lda.print_topics(num_topics=3, num_words=3))\n",
    "\n",
    "\"\"\"\n",
    "[\n",
    "(0, '0.071*\"water\" + 0.025*\"state\" + 0.025*\"three\"'), \n",
    "(1, '0.030*\"still\" + 0.028*\"hour\" + 0.026*\"sleeping\"'), \n",
    "(2, '0.073*\"percent\" + 0.069*\"water\" + 0.031*\"rain\"')\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint\n",
    "\n",
    "# Example news articles dataset\n",
    "news_articles = [\n",
    "    \"President Biden signed an executive order aimed at promoting competition in the US economy. The order includes measures to tackle corporate consolidation, improve competition in labor markets, and reduce barriers to entry for small businesses.\",\n",
    "    \n",
    "    \"Scientists have discovered a new species of butterfly in the Amazon rainforest. The butterfly, named Morpho helenor, has iridescent blue wings and is the first new species of butterfly found in the region in over a decade.\",\n",
    "    \n",
    "    \"The stock market experienced a sharp decline today, with major indices falling by over 2%. Analysts attribute the drop to concerns over rising inflation and the Federal Reserve's plans to tighten monetary policy.\",\n",
    "    \n",
    "    \"A new study suggests that regular exercise may help to reduce the risk of developing Alzheimer's disease. The study, published in the Journal of Neurology, found that people who engaged in physical activity had lower levels of amyloid-beta, a protein associated with Alzheimer's.\",\n",
    "    \n",
    "    \"Tesla CEO Elon Musk announced plans to build a new Gigafactory in Texas. The factory, which will produce batteries for electric vehicles, is expected to create thousands of jobs and further expand Tesla's manufacturing capacity.\"\n",
    "]\n",
    "\n",
    "# Tokenize the news articles (split into words)\n",
    "tokenized_articles = [article.lower().split() for article in news_articles]\n",
    "\n",
    "# Create a dictionary from the tokenized articles\n",
    "dictionary = corpora.Dictionary(tokenized_articles)\n",
    "\n",
    "# Create a corpus where each document is represented as a bag-of-words\n",
    "corpus = [dictionary.doc2bow(article) for article in tokenized_articles]\n",
    "\n",
    "# Specify the number of topics\n",
    "num_topics = 3\n",
    "\n",
    "# Build the LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10)\n",
    "\n",
    "# Print the topics and their top words\n",
    "pprint(lda_model.print_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New article\n",
    "new_article = \"A study reveals the benefits of meditation for mental health. Researchers found that regular meditation practice can reduce stress and anxiety levels.\"\n",
    "\n",
    "# Tokenize the new article\n",
    "tokenized_new_article = new_article.lower().split()\n",
    "\n",
    "# Convert the tokenized new article into bag-of-words representation\n",
    "new_article_bow = dictionary.doc2bow(tokenized_new_article)\n",
    "\n",
    "# Infer the topic distribution for the new article\n",
    "topic_distribution = lda_model.get_document_topics(new_article_bow)\n",
    "\n",
    "# Assign the new article to the topic with the highest probability\n",
    "max_prob_topic = max(topic_distribution, key=lambda x: x[1])\n",
    "\n",
    "# Print the assigned topic\n",
    "print(\"Assigned Topic:\", max_prob_topic[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Example news articles dataset\n",
    "news_articles = [\n",
    "    \"President Biden signed an executive order aimed at promoting competition in the US economy. The order includes measures to tackle corporate consolidation, improve competition in labor markets, and reduce barriers to entry for small businesses.\",\n",
    "    \"Scientists have discovered a new species of butterfly in the Amazon rainforest. The butterfly, named Morpho helenor, has iridescent blue wings and is the first new species of butterfly found in the region in over a decade.\",\n",
    "    \"The stock market experienced a sharp decline today, with major indices falling by over 2%. Analysts attribute the drop to concerns over rising inflation and the Federal Reserve's plans to tighten monetary policy.\",\n",
    "    \"A new study suggests that regular exercise may help to reduce the risk of developing Alzheimer's disease. The study, published in the Journal of Neurology, found that people who engaged in physical activity had lower levels of amyloid-beta, a protein associated with Alzheimer's.\",\n",
    "    \"Tesla CEO Elon Musk announced plans to build a new Gigafactory in Texas. The factory, which will produce batteries for electric vehicles, is expected to create thousands of jobs and further expand Tesla's manufacturing capacity.\"\n",
    "]\n",
    "\n",
    "# Tokenize the news articles\n",
    "tokenized_articles = [article.lower().split() for article in news_articles]\n",
    "\n",
    "# Create a dictionary from the tokenized articles\n",
    "dictionary = corpora.Dictionary(tokenized_articles)\n",
    "\n",
    "# Create a bag-of-words representation of each article\n",
    "corpus = [dictionary.doc2bow(article) for article in tokenized_articles]\n",
    "\n",
    "# Train an LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, passes=10)\n",
    "\n",
    "# Define the labels for each topic\n",
    "topic_labels = {\n",
    "    0: \"politics\",\n",
    "    1: \"economy\",\n",
    "    2: \"tech\"\n",
    "}\n",
    "\n",
    "# Assign topics to articles and output the results\n",
    "for i, article_bow in enumerate(corpus):\n",
    "    topic_distribution = lda_model.get_document_topics(article_bow)\n",
    "    max_prob_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
    "    assigned_topic = topic_labels[max_prob_topic]\n",
    "    \n",
    "    print(f\"Article {i+1}: Topic - {assigned_topic}\")\n",
    "    print(\"Top 5 words:\", [word for word, prob in lda_model.show_topic(max_prob_topic, topn=5)])\n",
    "    print(\"Article:\", news_articles[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# Assume `documents` is your list of news articles\n",
    "texts = [[word for word in document.lower().split()] for document in documents]\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Convert dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Apply LDA\n",
    "lda = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
    "\n",
    "# Extract top 5 words for each topic\n",
    "topics = lda.print_topics(num_words=5)\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, stem_text\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'D://APU Degree//Degree Sem 4//Investigations in Data Analytics//Assignment Work Done//3 IR Report//Dataset//Articles.csv'\n",
    "articles_df = pd.read_csv(file_path)\n",
    "\n",
    "# Define a preprocessing function that applies multiple preprocessing steps\n",
    "def preprocess_text(text):\n",
    "    custom_filters = [strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, stem_text]\n",
    "    return preprocess_string(text, custom_filters)\n",
    "\n",
    "# Apply preprocessing to each article\n",
    "articles_df['processed_text'] = articles_df['Article text'].apply(preprocess_text)\n",
    "\n",
    "# Create a dictionary from the processed text\n",
    "dictionary = corpora.Dictionary(articles_df['processed_text'])\n",
    "\n",
    "# Filter out extremes to remove too frequent and too rare words\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# Create a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in articles_df['processed_text']]\n",
    "\n",
    "# Running LDA model\n",
    "num_topics = 5\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=42)\n",
    "\n",
    "# Display the topics\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_categorize_new_document(new_document):\n",
    "    # Preprocess the new document\n",
    "    processed_doc = preprocess_text(new_document)\n",
    "    \n",
    "    # Convert to document-term matrix\n",
    "    doc_bow = dictionary.doc2bow(processed_doc)\n",
    "    \n",
    "    # Use the LDA model to get the topic distribution\n",
    "    doc_topics = lda_model.get_document_topics(doc_bow)\n",
    "    \n",
    "    # Sort the topics by probability\n",
    "    doc_topics_sorted = sorted(doc_topics, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the most dominant topic\n",
    "    dominant_topic = doc_topics_sorted[0] if doc_topics_sorted else None\n",
    "    \n",
    "    return dominant_topic\n",
    "\n",
    "# Example usage with a new document\n",
    "new_document = \"The latest international news, featuring top stories from around the world and breaking news, as it happens.\"\n",
    "dominant_topic = preprocess_and_categorize_new_document(new_document)\n",
    "\n",
    "# Print the most dominant topic and its probability\n",
    "print(f\"Dominant Topic: {dominant_topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, stem_text\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'D://APU Degree//Degree Sem 4//Investigations in Data Analytics//Assignment Work Done//3 IR Report//Dataset//Articles.csv'\n",
    "articles_df = pd.read_csv(file_path)\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_text(text):\n",
    "    custom_filters = [strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, stem_text]\n",
    "    return preprocess_string(text, custom_filters)\n",
    "\n",
    "# Apply preprocessing\n",
    "articles_df['processed_text'] = articles_df['Article text'].apply(preprocess_text)\n",
    "\n",
    "# Create a dictionary\n",
    "dictionary = corpora.Dictionary(articles_df['processed_text'])\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# Create a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in articles_df['processed_text']]\n",
    "\n",
    "# Run LDA\n",
    "num_topics = 5\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=42)\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# Function to preprocess and categorize new documents\n",
    "def preprocess_and_categorize_new_document(new_document):\n",
    "    processed_doc = preprocess_text(new_document)\n",
    "    doc_bow = dictionary.doc2bow(processed_doc)\n",
    "    doc_topics = lda_model.get_document_topics(doc_bow)\n",
    "    doc_topics_sorted = sorted(doc_topics, key=lambda x: x[1], reverse=True)\n",
    "    dominant_topic = doc_topics_sorted[0] if doc_topics_sorted else None\n",
    "    return dominant_topic\n",
    "\n",
    "# Example of categorizing a new document\n",
    "new_document = \"The latest international news, featuring top stories from around the world and breaking news, as it happens.\"\n",
    "dominant_topic = preprocess_and_categorize_new_document(new_document)\n",
    "\n",
    "# Print the most dominant topic and its probability\n",
    "print(f\"Dominant Topic: {dominant_topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your dataset is loaded into a DataFrame df\n",
    "# Replace 'path_to_your_dataset.csv' with your actual file path or use pd.read_excel() for Excel files\n",
    "# df = pd.read_csv('path_to_your_dataset.csv')\n",
    "\n",
    "# Example DataFrame creation (replace with your actual data loading code)\n",
    "data = {\n",
    "    'text': ['cnn breaking news', 'some text with cnn mentioned', 'no cnn here'],\n",
    "    'headline': ['headline 1', 'headline 2', 'headline 3'],\n",
    "    'category': ['news', 'news', 'other']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to remove 'cnn' from the 'text' column\n",
    "def remove_cnn(text):\n",
    "    return text.replace('cnn', '')\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "df['text'] = df['text'].apply(remove_cnn)\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Sample headlines\n",
    "headlines = [\n",
    "    \"Stocks soar to record highs\",\n",
    "    \"Company reports disappointing earnings\",\n",
    "    \"New product launch receives mixed reviews\",\n",
    "    \"Economic outlook remains uncertain\",\n",
    "    \"Innovative technology set to disrupt industry\",\n",
    "    \"Market volatility causes investor anxiety\",\n",
    "    \"Company announces major breakthrough\",\n",
    "    \"Natural disaster impacts local economy\",\n",
    "    \"Positive quarter boosts investor confidence\",\n",
    "    \"Regulatory changes create market uncertainty\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(headlines, columns=['headline'])\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "df['headline'] = df['headline'].apply(preprocess_text)\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X = vectorizer.fit_transform(df['headline'])\n",
    "\n",
    "# Apply K-means clustering\n",
    "num_clusters = 3  # We aim for 3 clusters: positive, neutral, negative\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Function to create word cloud for each cluster\n",
    "def plot_word_cloud(cluster_num):\n",
    "    text = \" \".join(df[df['cluster'] == cluster_num]['headline'].values)\n",
    "    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud for Cluster {cluster_num}\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot word clouds for each cluster\n",
    "for cluster_num in range(num_clusters):\n",
    "    plot_word_cloud(cluster_num)\n",
    "\n",
    "# Manually label clusters based on word clouds\n",
    "cluster_labels = {0: 'positive', 1: 'neutral', 2: 'negative'}\n",
    "df['sentiment'] = df['cluster'].map(cluster_labels)\n",
    "\n",
    "# Display the labeled data\n",
    "print(df[['headline', 'sentiment']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snorkel\n",
      "  Using cached snorkel-0.10.0-py3-none-any.whl (103 kB)\n",
      "Requirement already satisfied: munkres>=1.0.6 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from snorkel) (1.1.4)\n",
      "Requirement already satisfied: numpy>=1.24.0 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from snorkel) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from snorkel) (1.10.1)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from snorkel) (1.5.3)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from snorkel) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.2 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from snorkel) (1.2.2)\n",
      "Requirement already satisfied: torch>=1.2.0 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from snorkel) (2.0.1)\n",
      "Requirement already satisfied: tensorboard>=2.13.0 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from snorkel) (2.13.0)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from snorkel) (4.25.3)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from snorkel) (2.8.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->snorkel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->snorkel) (2022.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.2->snorkel) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.2->snorkel) (2.2.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from tensorboard>=2.13.0->snorkel) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from tensorboard>=2.13.0->snorkel) (1.57.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from tensorboard>=2.13.0->snorkel) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from tensorboard>=2.13.0->snorkel) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from tensorboard>=2.13.0->snorkel) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from tensorboard>=2.13.0->snorkel) (2.29.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from tensorboard>=2.13.0->snorkel) (67.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from tensorboard>=2.13.0->snorkel) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from tensorboard>=2.13.0->snorkel) (2.2.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from tensorboard>=2.13.0->snorkel) (0.38.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from torch>=1.2.0->snorkel) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from torch>=1.2.0->snorkel) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from torch>=1.2.0->snorkel) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from torch>=1.2.0->snorkel) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from tqdm>=4.33.0->snorkel) (0.4.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.13.0->snorkel) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.13.0->snorkel) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.13.0->snorkel) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.13.0->snorkel) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.13.0->snorkel) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.13.0->snorkel) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.13.0->snorkel) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.13.0->snorkel) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.13.0->snorkel) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.13.0->snorkel) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from sympy->torch>=1.2.0->snorkel) (1.2.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.13.0->snorkel) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.13.0->snorkel) (3.2.2)\n",
      "Installing collected packages: snorkel\n",
      "Successfully installed snorkel-0.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ChokJoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#install needed packages\n",
    "!pip install snorkel\n",
    "!pip install textblob\n",
    "#import libraries and modules\n",
    "import io\n",
    "import pandas as pd\n",
    "#Snorkel\n",
    "from snorkel.labeling import LabelingFunction\n",
    "import re\n",
    "from snorkel.preprocess import preprocessor\n",
    "from textblob import TextBlob\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "from snorkel.labeling import labeling_function\n",
    "#NLP packages\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import nltk.tokenize\n",
    "punc = string.punctuation\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#Supervised learning\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "##Deep learning libraries and APIs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #uplaod the data from your local directory\n",
    "# uploaded = files.upload()\n",
    "# # store the dataset as a Pandas Dataframe\n",
    "# df = pd.read_csv(io.BytesIO(uploaded['data.csv']))\n",
    "# #conduct some data cleaning\n",
    "# df = df.drop(['publish_date', 'Unnamed: 2'], axis=1)\n",
    "# df = df.rename(columns = {'headline_text': 'text'})\n",
    "# df['text'] = df['text'].astype(str)\n",
    "# #check the data info\n",
    "# df.info()\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "df = pd.read_csv('Cleaned_News_Articles_Final2.csv')\n",
    "df['headline'] = df['headline'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define constants to represent the class labels :positive, negative, and abstain\n",
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "#define function which looks into the input words to represent a proper label\n",
    "def keyword_lookup(x, keywords, label):  \n",
    "    if any(word in x.text.lower() for word in keywords):\n",
    "        return label\n",
    "    return ABSTAIN\n",
    "#define function which assigns a correct label\n",
    "def make_keyword_lf(keywords, label=POSITIVE):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords[0]}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label))\n",
    "#resource: https://www.snorkel.org/use-cases/01-spam-tutorial#3-writing-more-labeling-functions\n",
    "#these two lists can be further extended \n",
    "\"\"\"positive news might contain the following words' \"\"\"\n",
    "keyword_positive = make_keyword_lf(keywords=['boosts', 'great', 'develops', 'promising', 'ambitious', 'delighted', 'record', 'win', 'breakthrough', 'recover', 'achievement', 'peace', 'party', 'hope', 'flourish', 'respect', 'partnership', 'champion', 'positive', 'happy', 'bright', 'confident', 'encouraged', 'perfect', 'complete', 'assured' ])\n",
    "\"\"\"negative news might contain the following words\"\"\"\n",
    "keyword_negative = make_keyword_lf(keywords=['war','solidiers', 'turmoil', 'injur','trouble', 'aggressive', 'killed', 'coup', 'evasion', 'strike', 'troops', 'dismisses', 'attacks', 'defeat', 'damage', 'dishonest', 'dead', 'fear', 'foul', 'fails', 'hostile', 'cuts', 'accusations', 'victims',  'death', 'unrest', 'fraud', 'dispute', 'destruction', 'battle', 'unhappy', 'bad', 'alarming', 'angry', 'anxious', 'dirty', 'pain', 'poison', 'unfair', 'unhealthy'\n",
    "                                              ], label=NEGATIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up a preprocessor function to determine polarity & subjectivity using textlob pretrained classifier \n",
    "@preprocessor(memoize=True)\n",
    "def textblob_sentiment(x):\n",
    "    scores = TextBlob(x.text)\n",
    "    x.polarity = scores.sentiment.polarity\n",
    "    x.subjectivity = scores.sentiment.subjectivity\n",
    "    return x\n",
    "#find polarity\n",
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_polarity(x):\n",
    "    return POSITIVE if x.polarity > 0.6 else ABSTAIN\n",
    "#find subjectivity \n",
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_subjectivity(x):\n",
    "    return POSITIVE if x.subjectivity >= 0.5 else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41795/41795 [02:48<00:00, 248.44it/s]\n",
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "  0%|          | 0/100 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=2.663]\n",
      "  9%|▉         | 9/100 [00:00<00:01, 88.25epoch/s]INFO:root:[10 epochs]: TRAIN:[loss=0.923]\n",
      "INFO:root:[20 epochs]: TRAIN:[loss=0.341]\n",
      "INFO:root:[30 epochs]: TRAIN:[loss=0.006]\n",
      "INFO:root:[40 epochs]: TRAIN:[loss=0.044]\n",
      "INFO:root:[50 epochs]: TRAIN:[loss=0.014]\n",
      "INFO:root:[60 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[70 epochs]: TRAIN:[loss=0.002]\n",
      " 71%|███████   | 71/100 [00:00<00:00, 396.19epoch/s]INFO:root:[80 epochs]: TRAIN:[loss=0.001]\n",
      "INFO:root:[90 epochs]: TRAIN:[loss=0.000]\n",
      "100%|██████████| 100/100 [00:00<00:00, 406.53epoch/s]\n",
      "INFO:root:Finished Training\n"
     ]
    }
   ],
   "source": [
    "#combine all the labeling functions \n",
    "lfs = [keyword_positive, keyword_negative, textblob_polarity, textblob_subjectivity ]\n",
    "#apply the lfs on the dataframe\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_snorkel = applier.apply(df=df)\n",
    "#apply the label model\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "#fit on the data\n",
    "label_model.fit(L_snorkel)\n",
    "#predict and create the labels\n",
    "df[\"label\"] = label_model.predict(L=L_snorkel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique count in category column:\n",
      " 0    37700\n",
      " 1     3349\n",
      "-1      746\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print each unique counts of the column (target values)\n",
    "print(\"Unique count in category column:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out unlabeled data points\n",
    "df= df.loc[df.label.isin([0,1]), :]\n",
    "#find the label counts \n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "                                              0.0/126.0 kB ? eta -:--:--\n",
      "     -------------------------------------  122.9/126.0 kB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 126.0/126.0 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chokjoe\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2024.6.2)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "                                                text sentiment\n",
      "0  I love this product! It works great and is ver...  positive\n",
      "1      This is the worst experience I have ever had.  negative\n",
      "2  The product is okay, not too bad but not great...  negative\n",
      "3    Absolutely fantastic! Exceeded my expectations.  positive\n",
      "4  I am not sure how I feel about this, it’s just...  negative\n",
      "5             Terrible! I will never buy this again.  negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\ChokJoe\\AppData\\Roaming\\Python\\Python311\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "!pip install vaderSentiment\n",
    "\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Sample data (replace this with your actual data)\n",
    "data = {\n",
    "    'text': [\n",
    "        'I love this product! It works great and is very affordable.',\n",
    "        'This is the worst experience I have ever had.',\n",
    "        'The product is okay, not too bad but not great either.',\n",
    "        'Absolutely fantastic! Exceeded my expectations.',\n",
    "        'I am not sure how I feel about this, it’s just average.',\n",
    "        'Terrible! I will never buy this again.'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a function to classify sentiment\n",
    "def classify_sentiment(text):\n",
    "    # Get the sentiment scores\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    # Get the compound score\n",
    "    compound = scores['compound']\n",
    "    # Classify the sentiment based on the compound score\n",
    "    if compound >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply the sentiment classification function to the DataFrame\n",
    "df['sentiment'] = df['text'].apply(classify_sentiment)\n",
    "\n",
    "# Display the DataFrame with sentiment labels\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive    37192\n",
      "neutral      2546\n",
      "negative     2057\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ChokJoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ChokJoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# load the dataset\n",
    "df = pd.read_csv('aug_final.csv')\n",
    "\n",
    "# tokenize text\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization to the text column in the DataFrame\n",
    "df['text'] = df['text'].apply(tokenize_text)\n",
    "\n",
    "# Remove any non-English words\n",
    "english_words = set(words.words()) \n",
    "def remove_non_english(tokens):\n",
    "    english_tokens = []\n",
    "    for word in tokens:\n",
    "        if word in english_words:\n",
    "            english_tokens.append(word)\n",
    "        else:\n",
    "            english_tokens.append('')\n",
    "    return [token for token in english_tokens if token != '']\n",
    "df['text'] = df['text'].apply(remove_non_english)\n",
    "\n",
    "df['text'] = df['text'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "df.to_csv(\"aug_final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChokJoe\\AppData\\Local\\Temp\\ipykernel_18548\\2763808530.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  downsampled_data = df.groupby('category').apply(lambda x: x.sample(n=min(upper_limit, max(lower_limit, len(x))), random_state=42))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "category\n",
       "business         1000\n",
       "entertainment    1000\n",
       "health           1000\n",
       "news             1000\n",
       "opinions         1000\n",
       "politics         1000\n",
       "sport            1000\n",
       "us               1000\n",
       "world            1000\n",
       "weather           570\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv('augted_test.csv')  \n",
    "\n",
    "# select 10000 data for each sentiment\n",
    "lower_limit = 100\n",
    "upper_limit = 1000\n",
    "\n",
    "# grp the data by sentiment and perform downsampling within each grp\n",
    "downsampled_data = df.groupby('category').apply(lambda x: x.sample(n=min(upper_limit, max(lower_limit, len(x))), random_state=42))\n",
    "\n",
    "downsampled_data = downsampled_data.reset_index(drop=True)\n",
    "\n",
    "downsampled_data.to_csv('balanced_category_test.csv', index=False)\n",
    "downsampled_data['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv('augted_test.csv') \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
